{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBLP Publication Key Data Spreadsheet Conversion\n",
    "\n",
    "We will be parsing the DBLP XML file for research publication records and extracting the title, conference, year published, and author tags\n",
    "With each publication, the script will determine the following:\n",
    "1. The author(s) affiliations\n",
    "2. The country in which the affiliated institutions are located (abbreviated)\n",
    "3. The geographic region in which the affiliated institutions are located (ie. asia, europe, north america)\n",
    "\n",
    "A final csv including the author, conference, date, school and location details of each article will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and file paths\n",
    "\n",
    "# Import libs\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input XML file path, change to your own path\n",
    "dblp_data = 'C:/Eric/Projects/datasets/uniranking_data/dblp-2024-05-02.xml'\n",
    "\n",
    "# DTD file path\n",
    "dtd_path = 'C:/Eric/Projects/datasets/uniranking_data/dblp.dtd'\n",
    "\n",
    "# Reference data\n",
    "locations = pd.read_csv('C:/Eric/Projects/AI_Rankings/data/country-info.csv')\n",
    "csr = pd.read_csv('C:/Eric/Projects/AI_Rankings/data/csrankings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set of conferences used by CSRankings\n",
    "conf_set = {'aaai','acl','asplos','ase',\n",
    " 'bioinformatics',\n",
    " 'cav','ccs','ccs2','cgf','chi','corr','crypto','cscw','csl','cvpr',\n",
    " 'dac',\n",
    " 'ec','eccv','emnlp','emsoft','entcs','eurocrypt','eurosys',\n",
    " 'fast','focs',\n",
    " 'hpca','hpdc','huc',\n",
    " 'iccad','iccad2','iccv','icde','icfp','iclr','icml','icra','ics','icse','ijcai','ieeessp',\n",
    " 'imc','imc2','imwut','innovations','intcompsymp','ipps','iros','isca','ismb','issta',\n",
    " 'kbse','kdd',\n",
    " 'lics',\n",
    " 'micro','mobicom','mobisys',\n",
    " 'naacl','ndss','nips','nsdi','neurips',\n",
    " 'oopsla','osdi',\n",
    " 'pacmmod','pacmpl','pervasive','pet','pldi','podc','pods','pomacs','popets','popl','pvldb',\n",
    " 'recomb','rss','rtas','rtss',\n",
    " 'sc','sensys','sigcomm','sigcse','sigecom','siggraph','sigir',\n",
    " 'sigmetrics','sigmod','sigsoft','soda','sosp','sp','spaa','stoc',\n",
    " 'tcad','tecs','tog','tvcg',\n",
    " 'uist','usenix','uss',\n",
    " 'visualization','vldb','vr',\n",
    " 'wdag','wine','wsdm','www'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing.\n",
      "Found 411568 papers.\n"
     ]
    }
   ],
   "source": [
    "# Parse the XML file\n",
    "\n",
    "def str_diff(page_num:str):\n",
    "    try:\n",
    "        p = page_num.split('-') #if pages in the format 'xxx-xxx'\n",
    "        try:                    #if pages in the format 'xxx:xxx'\n",
    "            p[0] = p[0].split(':')[0]\n",
    "            p[1] = p[1].split(':')[0]\n",
    "        except:\n",
    "            pass\n",
    "        n = int(p[1]) - int(p[0])\n",
    "    except: #if pages in the format 'xxx'\n",
    "        n = 1\n",
    "    return n\n",
    "\n",
    "# Initialize a DataFrame\n",
    "cols = ['Title','Conference','Year','Authors']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "rows = []\n",
    "\n",
    "num_papers = 0\n",
    "# Parse the XML file elementwise\n",
    "context = etree.iterparse(dblp_data, events=('end',), tag='inproceedings', dtd_validation=True, load_dtd=True, huge_tree=True)\n",
    "for event, paper in context:\n",
    "    try:\n",
    "        conf = paper.attrib['key'].split('/')[1]\n",
    "        page_num = paper.find(\"pages\").text\n",
    "        pages = str_diff(page_num) if '-' in page_num else 1\n",
    "\n",
    "        if conf not in conf_set:\n",
    "        #if conf not in conf_set or pages < 6: # if we want to specify a minimum number of pages\n",
    "            continue\n",
    "        \n",
    "        num_papers += 1\n",
    "        title = paper.find(\"title\").text\n",
    "        year = paper.find(\"year\").text\n",
    "        authors = [author.text for author in paper.findall(\"author\")]\n",
    "        row = {'Title': title, \n",
    "            'Conference': conf, \n",
    "            'Year': year,\n",
    "            'Authors': authors}\n",
    "        rows.append(row)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Free up memory\n",
    "    paper.clear()\n",
    "    while paper.getprevious() is not None:\n",
    "        del paper.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "print(\"Finished parsing.\")\n",
    "print(f\"Found {num_papers} papers.\")\n",
    "\n",
    "# Concatenate the rows to the DataFrame\n",
    "rows_df = pd.DataFrame(rows)\n",
    "publications = pd.concat([df, rows_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411035/411035 [2:20:46<00:00, 48.66it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining publications: 226718\n"
     ]
    }
   ],
   "source": [
    "# gather affiliation and location info from author data\n",
    "inst_list = []\n",
    "country_list = []\n",
    "region_list = []\n",
    "# find country of list of institutions   \n",
    "def find_inst_country(country_info:pd.DataFrame,inst_list:list):\n",
    "    countries = []\n",
    "    for inst in inst_list:\n",
    "        country = country_info[country_info['institution'] == inst]['countryabbrv']\n",
    "        if not country.empty:\n",
    "            countries.append(country.iloc[0])\n",
    "        else:\n",
    "            countries.append('us')\n",
    "    return set(countries)\n",
    "\n",
    "# find multiple author affiliation\n",
    "def find_author_affil(authors:list,csr:pd.DataFrame):\n",
    "    affils = []\n",
    "    for author in authors:\n",
    "        try:\n",
    "            inst = csr[csr['name'] == author]['affiliation']\n",
    "            affils.append(inst.iloc[0])\n",
    "        except:\n",
    "            continue\n",
    "    return set(affils)\n",
    "\n",
    "# find region of list of institutions   \n",
    "def find_country_region(country_info:pd.DataFrame,country_list:list):\n",
    "    regs = []\n",
    "    for c in country_list:\n",
    "        reg = country_info[country_info['countryabbrv'] == c]['region']\n",
    "        if not reg.empty:\n",
    "            regs.append(reg.iloc[0])\n",
    "        else:\n",
    "            regs.append('us')\n",
    "    return set(regs)\n",
    "\n",
    "# map location data of institutions\n",
    "for _, row in tqdm(publications.iterrows(), total=publications.shape[0]):\n",
    "    authors = row['Authors']\n",
    "\n",
    "    affils = find_author_affil(authors,csr)\n",
    "    inst_list.append(affils)\n",
    "\n",
    "    countries = find_inst_country(locations,affils)\n",
    "    country_list.append(countries)\n",
    "\n",
    "    region = find_country_region(locations,countries)\n",
    "    region_list.append(region)\n",
    "\n",
    "# Add the affiliation and location info to the DataFrame\n",
    "publications['Affiliations'] = inst_list\n",
    "publications['Countries'] = country_list\n",
    "publications['Region'] = region_list\n",
    "\n",
    "# Since we are only interested in publications authored by faculty from academic institutions,\n",
    "# we will filter out publications by authors with no affiliated institutions at the moment (corporate researchers, etc.)\n",
    "result = publications[publications['Affiliations'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "print('Remaining publications:', len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Region'] = result['Region'].apply(lambda s: replace_set(s, 'us', 'northamerica'))\n",
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Region'] = result['Region'].apply(lambda s: replace_set(s, 'canada', 'northamerica'))\n",
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Affiliations'] = result['Affiliations'].apply(', '.join)\n",
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Countries'] = result['Countries'].apply(', '.join)\n",
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Region'] = result['Region'].apply(', '.join)\n",
      "C:\\Users\\erics\\AppData\\Local\\Temp\\ipykernel_30724\\3408209727.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Authors'] = result['Authors'].apply(join_authors)\n"
     ]
    }
   ],
   "source": [
    "# spreadsheet formatting\n",
    "def replace_set(s, country, region):\n",
    "    if country in s:\n",
    "        s.remove(country)\n",
    "        s.add(region)\n",
    "    return s\n",
    "\n",
    "def join_authors(authors):\n",
    "    return ', '.join(authors) if isinstance(authors, list) else authors\n",
    "\n",
    "# the default region labling by csranking lists us and canada individually\n",
    "# we will group them together into northamerica for consistency\n",
    "if any('us' in s for s in result['Region']):\n",
    "    #result['Region'] = result['Region'].apply(ast.literal_eval)\n",
    "    result['Region'] = result['Region'].apply(lambda s: replace_set(s, 'us', 'northamerica'))\n",
    "    result['Region'] = result['Region'].apply(lambda s: replace_set(s, 'canada', 'northamerica'))\n",
    "\n",
    "# only for formatting purposes we will remove the [] and {} from the sets\n",
    "# by joining these sets as one concatenated string\n",
    "result['Affiliations'] = result['Affiliations'].apply(', '.join)\n",
    "result['Countries'] = result['Countries'].apply(', '.join)\n",
    "result['Region'] = result['Region'].apply(', '.join)\n",
    "result['Authors'] = result['Authors'].apply(join_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory usage: 121.13115 MB\n"
     ]
    }
   ],
   "source": [
    "# save as csv\n",
    "filename = \"DBLP_publications.csv\"\n",
    "result.to_csv(filename,index=False)\n",
    "\n",
    "# check memory usage\n",
    "total_memory_usage = result.memory_usage(deep=True).sum()\n",
    "print(\"Total memory usage:\", total_memory_usage/1000000, \"MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
